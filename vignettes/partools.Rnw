
\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.25in}
\setlength{\parindent}{0in}
\setlength{\parskip}{2mm}

\usepackage{times}
\usepackage{graphicx}

% library(knitr)
%\VignetteIndexEntry{Partools}

\title{partools: a Sensiible Package for Large Data Sets}

\author{Norm Matloff, with Contributions form Alex Rumbaugh}

\begin{document}

\maketitle

With the advent of Big Data, the Hadoop framework became ubiquitous.
Yet it was clear that Hadoop had major shortcomings, and recently these
are being much more seriously discussed.\footnote{See for example ``The
Hadoop Honeymoon is Over,'' {\it
https://www.linkedin.com/pulse/hadoop-honeymoon-over-martyn-jones}} This
has resulted in a new platform, Spark, gaining popularity.  As with
Hadoop, there is an R interfaces available for Spark, named SparkR. 

Spark overcomes one of Hadoop's major problems, which is the lack of
ability to cache data in a multi-pass computation.  However, Spark
unfortunately retains the drawbacks of Hadoop:

\begin{itemize}

\item Due to reliance on sophisticated infrastructure involving Java and
Scalar, they are difficult to install and properly configure for those
who are not computer systems experts. 

\item A major plus for Hadoop/Spark, fault tolerance, is needed only for
users working on extremely large clusters, consisting of hundreds or
thousands of nodes. Disk failure rates are simply too low for fault
tolerance to be an issue on other systems.\footnote{In addition, one can
provide fault tolerance externally, say with the XtreemFS system.}
Worse, the fault tolerance mechanisms, which are especially extensive in
Spark, {\it slow down the computation}.

\end{itemize}

The one firm advantage of Hadoop/Spark is their use of distributed file
systems.  Under the philosophy ``Move the computation to the data,
rather than {\it vice versa},'' network traffic may be greatly reduced,
thus speeding up computation.

Therefore:

\begin{quote}
It makes sense to develop and use a package that retains the
distributed-file nature of Hadoop/Spark  while staying fully within 
the simple-yet-more-powerful R framework.  The {\bf partools} package
is designed to meet these goals.
\end{quote}

Since {\bf partools} uses the portion of the R {\bf parallel} package
derived from the package {\bf snow}, and because it is meant as an
alternative to Hadoop, we informally refer to {\bf partools} as
Snowdoop.

\section{Overview of the partools Package}

The package is based on the following very simple principles:

\begin{itemize}

\item Files are stored in a distributed manner, in files with a common
basename. For example, the file {\bf x} is stored as separate files {\bf
x.01}, {\bf x.02} etc. 

\item Data frames and matrices are stored in memory at the nodes in a
distributed manner, with a common name.  For example, the data frame
{\bf y} is stored in chunks at the cluster nodes, each chunk known as {\bf
y} at its node.

\end{itemize}

\subsection{Statistical Functions vs. Tabulative Functions}

One important point is the distinction between {\it statistical}
functions and {\it tabulative} ones.

A statistical function is inherently an approximation.  An estimated
regression coefficient $\widehat{\beta}_i$ is subject to sampling
variability, as reflected in the standard error reported in the output
of {\bf lm()}.  With a large data set, the standard error may be small ---
or maybe not, if the number of predictor variables is large (which is
common in large data sets).

By contrast, tabulative functions must be exact.


\end{document}

