\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.25in}
\setlength{\parindent}{0in}
\setlength{\parskip}{2mm}

\usepackage{times}
\usepackage{graphicx}

% library(knitr)
%\VignetteIndexEntry{Partools}

\title{partools: a Sensiible Package for Large Data Sets}

\author{Norm Matloff, with Contributions from Alex Rumbaugh}

\begin{document}

\maketitle

With the advent of Big Data, the Hadoop framework became ubiquitous.
Yet it was clear that Hadoop had major shortcomings, and recently these
are being much more seriously discussed.\footnote{See for example ``The
Hadoop Honeymoon is Over,'' {\it
https://www.linkedin.com/pulse/hadoop-honeymoon-over-martyn-jones}} This
has resulted in a new platform, Spark, gaining popularity.  As with
Hadoop, there is an R interfaces available for Spark, named SparkR. 

Spark overcomes one of Hadoop's major problems, which is the lack of
ability to cache data in a multi-pass computation.  However, Spark
unfortunately retains the drawbacks of Hadoop:

\begin{itemize}

\item Due to reliance on sophisticated infrastructure involving Java and
Scalar, they are difficult to install and properly configure for those
who are not computer systems experts. 

\item A major plus for Hadoop/Spark, fault tolerance, is needed only for
users working on extremely large clusters, consisting of hundreds or
thousands of nodes. Disk failure rates are simply too low for fault
tolerance to be an issue on other systems.\footnote{In addition, one can
provide fault tolerance externally, say with the XtreemFS system.}
Worse, the fault tolerance mechanisms, which are especially extensive in
Spark, {\it slow down the computation}.

\end{itemize}

The one firm advantage of Hadoop/Spark is their use of distributed file
systems.  Under the philosophy ``Move the computation to the data,
rather than {\it vice versa},'' network traffic may be greatly reduced,
thus speeding up computation.

Therefore:

\begin{quote}
It makes sense to develop and use a package that retains the
distributed-file nature of Hadoop/Spark  while staying fully within 
the simple-yet-more-powerful R framework.  The {\bf partools} package
is designed to meet these goals.
\end{quote}

Since {\bf partools} uses the portion of the R {\bf parallel} package
derived from the package {\bf snow}, and because it is meant as an
alternative to Hadoop, we informally refer to {\bf partools} as
Snowdoop.

\section{Overview of the partools Package}

The package is based on the following very simple principles:

\begin{itemize}

\item Files are stored in a distributed manner, in files with a common
basename. For example, the file {\bf x} is stored as separate files {\bf
x.01}, {\bf x.02} etc. 

\item Data frames and matrices are stored in memory at the nodes in a
distributed manner, with a common name.  For example, the data frame
{\bf y} is stored in chunks at the cluster nodes, each chunk known as {\bf
y} at its node.

\end{itemize}

\subsection{Package Structure}

To understand the function structure, and indeed the central philosophy,
keep in mind the following definitions, for a cluster in the R {\bf
parallel} package.

\begin{itemize}

\item A {\it distributed file} {\bf x} consists of a number of separate
physical files, with names {\bf x.01}, {\bf x.02} and so
on.\footnote{The number of leading 0s in the suffix will depend on the
number of cluster nodes.}  The file {\bf x} consists of the {\bf
rbind()} of the rows of those separate files, though {\bf x} may be
virtual.  Cluster node {\bf i} is responsible for {\bf x.i}.

\item A {\it distributed data frame} (or distributed matrix) {\bf d}
consists of a number of separate data frames, one at each cluster node,
all named {\bf d}.  The data frame as a whole consists of the {\bf
rbind()} of the rows of those separate data frames, though it may be
virtual.

\end{itemize}

The fact that all the file chunks or data frame chunks have the same
name (or same prefix) plays a key role in the software.

The package consists of three main groups of functions:

\begin{itemize}

\item {\bf distributed-file functions:}

   \begin{itemize}

   \item {\bf filesplit():}  Create distributed file from monotlithic one.

   \item {\bf filecat():}  Create monotlithic file from distributed one.

   \item {\bf fileread():}  Read distributed file into distributed data
   frame.

   \item {\bf filesave():}  Write distributed data frame to distributed
   file.

   \end{itemize}

\item {\bf tabulative functions:}

\item {\bf statistical functions:}

\end{itemize}

There are also attendant support functions.

One important point is the distinction between {\it statistical}
functions and {\it tabulative} ones.

A statistical function is inherently an approximation.  An estimated
regression coefficient $\widehat{\beta}_i$ is subject to sampling
variability, as reflected in the standard error reported in the output
of {\bf lm()}.  With a large data set, the standard error may be small ---
or maybe not, if the number of predictor variables is large (which is
common in large data sets).

By contrast, tabulative functions must be exact.


\end{document}

