\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.25in}
\setlength{\parindent}{0in}
\setlength{\parskip}{2mm} 

% \usepackage{times}
\usepackage{graphicx}

% library(knitr)
%\VignetteIndexEntry{Partools}

\title{{\bf partools: a Sensiible Package for Large Data Sets}}

\author{Norm Matloff, with Contributions from Alex Rumbaugh}

\begin{document}

\maketitle

With the advent of Big Data, the Hadoop framework became ubiquitous.
Yet it was clear that Hadoop had major shortcomings, and recently these
are being much more seriously discussed.\footnote{See for example ``The
Hadoop Honeymoon is Over,'' {\it
https://www.linkedin.com/pulse/hadoop-honeymoon-over-martyn-jones}} This
has resulted in a new platform, Spark, gaining popularity.  As with
Hadoop, there is an R interfaces available for Spark, named SparkR. 

Spark overcomes one of Hadoop's major problems, which is the lack of
ability to cache data in a multi-pass computation.  However, Spark
unfortunately retains the drawbacks of Hadoop:

\begin{itemize}

\item Due to reliance on sophisticated infrastructure involving Java and
Scala, they are difficult to install and properly configure for those
who are not computer systems experts. 

\item A major plus for Hadoop/Spark, fault tolerance, is needed only for
users working on extremely large clusters, consisting of hundreds or
thousands of nodes. Disk failure rates are simply too low for fault
tolerance to be an issue on other systems.\footnote{In addition, one can
provide fault tolerance externally, say with the XtreemFS system.}
Worse, the fault tolerance mechanisms, which are especially extensive in
Spark, {\it slow down the computation}.

\end{itemize}

The one firm advantage of Hadoop/Spark is their use of distributed file
systems.  Under the philosophy ``Move the computation to the data,
rather than {\it vice versa},'' network traffic may be greatly reduced,
thus speeding up computation.

Therefore:

\begin{quote}
It makes sense to develop and use a package that retains the
distributed-file nature of Hadoop/Spark  while staying fully within 
the simple-yet-more-powerful R framework.  The {\bf partools} package
is designed to meet these goals.
\end{quote}

Since {\bf partools} uses the portion of the R {\bf parallel} package
derived from the package {\bf snow}, and because it is meant as an
alternative to Hadoop, we informally refer to {\bf partools} as
Snowdoop.

\section{Where Is the Magic?}

As you will see later, {\bf partools} can deliver some impressive
speedups.  But there is nothing magical about this.  Instead, the value
of the package consists of just two simple sources:

\begin{itemize}

\item [(a)] The package follows a philosophy of forming distributed
objects {\it and keeping using them in distributed form throughout one's
R session.}

\item [(b)] The package consists of a number of utility functions that
greatly facilitate creating, storing and {\it analyzing} distributed
objects.

\end{itemize}

\section{Overview of the partools Package}

The package is based on the following very simple principles:

\begin{itemize}

\item Files are stored in a distributed manner, in files with a common
basename. For example, the file {\bf x} is stored as separate files {\bf
x.01}, {\bf x.02} etc. 

\item Data frames and matrices are stored in memory at the nodes in a
distributed manner, with a common name.  For example, the data frame
{\bf y} is stored in chunks at the cluster nodes, each chunk known as {\bf
y} at its node.

\end{itemize}

\subsection{Package Structure}

To understand the function structure, and indeed the central philosophy,
keep in mind the following definitions, for a cluster in the R {\bf
parallel} package.

\begin{itemize}

\item A {\it distributed file} {\bf x} consists of a number of separate
physical files, with names {\bf x.01}, {\bf x.02} and so
on.\footnote{The number of leading 0s in the suffix will depend on the
number of cluster nodes.}  The file {\bf x} consists of the {\bf
rbind()} of the rows of those separate files, though {\bf x} may be
virtual.  Cluster node {\bf i} is responsible for {\bf x.i}.

\item A {\it distributed data frame} (or distributed matrix) {\bf d}
consists of a number of separate data frames, one at each cluster node,
all named {\bf d}.  The data frame as a whole consists of the {\bf
rbind()} of the rows of those separate data frames, though it may be
virtual.

\end{itemize}

The fact that all the file chunks or data frame chunks have the same
name (or same prefix) plays a key role in the software.

The package consists of three main groups of functions:

\begin{itemize}

\item {\bf distributed-file functions:}

   \begin{itemize}

   \item {\bf filesplit():}  Create distributed file from monotlithic one.

   \item {\bf filecat():}  Create monotlithic file from distributed one.

   \item {\bf fileread():}  Read distributed file into distributed data
   frame.

   \item {\bf readnscramble():}  Read distributed file into distributed
   data frame, but randomize the record order.

   \item {\bf filesave():}  Write distributed data frame to distributed
   file.

   \item {\bf filechunkname():} For the calling cluster node, returns
   the full name of the file chunk, including suffix, e.g. '01', '02'
   etc.

   \end{itemize}

\item {\bf tabulative functions:}

   \begin{itemize}

   \item {\bf distribsplit():}  Create distributed data frame/matrix
   from monotlithic one.

   \item {\bf distribcat():}  Create monotlithic data frame/matrix from
   distributed one.

   \item {\bf distribagg():}  Distributed form of R's {\bf aggregate()}.

   \item {\bf distribrange():}  Wrapper for {\bf distribagg()} to obtain
   cell counts.

   \item {\bf distribgetrows()}: Applies an R {\bf select()} or similar
   operation to the distributed object, and collects the results into a
   single object at the caller.

   \item {\bf distribrange():}  Distributed form of R's {\bf range()}.

   \end{itemize}

\item {\bf statistical functions:}

These use the Software Alchemy (SA) method ({\it Parallel Computation for
Data Science}, N. Matloff, Chapman and Hall, 2015.) to parallelize
statistical operations.  The resulting estimators have the same
statistical accuracy as the original serial ones.\footnote{In the world
of parallel computation, the standard word for nonparallel is {\it
serial}.}

   \begin{itemize}

   \item {\bf ca():} General SA algorithm.

   \item {\bf cabase():} Core of {\bf ca()}.

   \item {\bf calm():}  Wrapper for SA version of R's {\bf lm()}.

   \item {\bf caglm():}  Wrapper for SA version of R's {\bf glm()}.

   \item {\bf cakm():}  Wrapper for SA version of R's {\bf kmeans()}.

   \item {\bf cakm():}  Wrapper for SA version of R's {\bf aggregate()}.

   \end{itemize}

\end{itemize}

There are also attendant support functions.

\section{Sample Session}

Our data set, from
{\it http://stat-computing.org/dataexpo/2009/the-data.html} consists of
the well-known records of airline flight delay.
For convenience, we'll just use the data for 2008, which consists of
about 7 million records.  This is large enough to illustrate speedup
due to parallelism, but small enough that we won't have to wait really
long amounts of time in our sample session here.  

The session was run on a 16-core machine, with a 16-node {\bf parallel}
cluster.  Note carefully, though, that we should not expect a 16-fold
speedup. In the world of parallel computation, one usually gets of
speedups of considerably less than $n$ for a platform of $n$
computational entities.  Indeed, one is often saddened to find that the
parallel version is actually {\it slower} than the serial one!

The file, {\bf yr2008}, was first split into a distributed file, stored
in {\bf yr2008.01},...,{\bf yr2008.16}, using {\bf filesplitrand()}, and
then read into memory at the 16 cluster nodes using {\bf fileread()}:

\begin{verbatim}
> filesplitrand(cls,'yr2008','yr2008r',2,header=TRUE,sep=",")
> fileread(cls,'yr2008r','yr2008',2,header=TRUE, sep=",")
\end{verbatim}

The call to {\bf filesplitrand()} splits the file as described above;
since these files are permanent, we can skip this step in future R
sessions involving this data (if the file doesn't change).  The function
{\bf filesplitrand()} was used instead of {\bf filesplit()} to construct
the distributed file, in order to randomize the placement of the records
of {\bf yr2008} across cluster nodes.  The advantage of randomization
will be explained shortly.

In order to run timing comparisons, the full file was also read into
memory at the cluster manager:

\begin{verbatim}
> yr2008 <- read.csv("yr2008")
\end{verbatim}

The first operation run involved the package's distributed version of
R's {\bf aggregate()}.  Here we want to tabulate day of week, month and
whether the flight was canceled, broken down into cells according to
flight origin and destination.  We'll find the maximum value in each
cell.

\begin{verbatim}
> system.time(print(distribagg(cls, c("DepDelay","ArrDelay","AirTime"),
   c("Origin","Dest"),"yr2008", FUN="max")))  
...
5193    CDV  YAK      327      325      54
5194    JNU  YAK      317      308      77
5195    SLC  YKM      110      118     115
5196    IPL  YUM      162      163      26
...
   user  system elapsed  
  2.291   0.084  15.952 
\end{verbatim}

\begin{verbatim}
> system.time(print(aggregate(cbind(DepDelay,ArrDelay,AirTime) ~ 
   Origin+Dest,data=yr2008,FUN=max)))  
...
5193    CDV  YAK      327      325      54
5194    JNU  YAK      317      308      77
5195    SLC  YKM      110      118     115
5196    IPL  YUM      162      163      26
...
   user  system elapsed  
249.038   0.444 249.634
\end{verbatim}

So, the results of {\bf distribagg()} did indeed match those of {\bf
aggregate()}, but did so more than 15 times faster!

Remember, the key philosophy of {\bf partools} is to create distributed
objects and then keep using them in distributed form.  However, in some
cases, we may wish to collect a distributed result into a monolithic
object, especially if the result is small.  This is done in the next
example:

Say we wish to do a filter operation, extracting the data on all the
Sunday evening flights, and collect it into one place.   Here is the
direct version:

\begin{verbatim}
> sundayeve <- with(yr2008,yr2008[DayOfWeek==1 & DepTime > 1800,])
\end{verbatim}

This actually is not a time-consuming operation, but again, in typical
{\bf partools} use, we would only have the distributed version of {\bf
yr2008}.  Here is how we would achieve the same effect from the
distributed object:

\begin{verbatim}
> sundayeved <- distribgetrows(cls,'with(yr2008,yr2008[DayOfWeek==1 & DepTime > 1800,])')
\end{verbatim}

In, say, investigating data accuracy, we may wish to flag all records
having an inordinate number of NA values.  As I first step, we may wish
to add a column to our data frame, indicating how many NA values are in
each row.  If we did not have the advantage of distributed computation,
here is how long it would take for our flight delay data:

\begin{verbatim}
> sumna            
function(x) sum(is.na(x))
> system.time(yr2008$n1 <- apply(yr2008[,c(5,7,8,11:16,19:21)],1,sumna))
   user  system elapsed
268.463   0.773 269.542
\end{verbatim}

But it is of course much faster on a distributed basis, using the {\bf
parallel} package function {\bf clusterEvalQ()}:

\begin{verbatim}
> clusterExport(cls,"sumna",envir=environment())
> system.time(clusterEvalQ(cls,yr2008$n1 <- apply(yr2008[,c(5,7,8,11:16,19:21)],1,sumna)))
   user  system elapsed 
  0.094   0.012  16.758 
\end{verbatim}

The speedup here was about 16, fully utilizing all 16 cores.

Ordinarily, we would continue that NA analysis on a distributed basis,
in keeping with the {\bf partools} philosophy of setting up distributed
objects and then repeatedly dealing with them on a distributed basis.
If our subsequent operations continue to have time complexity linear in
the number of records processes, we should continue to get speedups of
about 16.

On the other hand, we may wish to gather together all the records have 8
or more NA values.  In the nonparallel context, it would take some time:

\begin{verbatim}
> system.time(na8 <- yr2008[yr2008$n1 > 7,])
   user  system elapsed 
  9.292   0.028   9.327 
\end{verbatim}

In the distributed manner, it is slightly faster:

\begin{verbatim}
> system.time(na8d <- distribgetrows(cls,'yr2008[yr2008$n1 > 7,]'))
   user  system elapsed 
  5.524   0.160   6.584 
\end{verbatim}

The speedup is less here, as the resulting data must travel from the
cluster nodes to the cluster manager. In this case, this is just a
memory-to-memory transfer rather than across a netwrok, as we are on a
multicore machine, but it still takes time.  If the number of records
satisfying the filtering condition had been smaller than the 136246 we
had here, the speedup factor would have been greater.

Now let's turn to statistical operations, starting of course with linear
regression.  As noted, these {\bf partools} functions make use of
Software Alchemy, which replaces the given operation by a {\it
distributed, statistically equivalent} operation.  This will often
produce a significant speedup.  Note again that though the result may
different from the non-distributed version, say in the third significant
digit, it is just as accurate statistically.

In the flight data, we predicted the arrival delay from the departure
delay and distance, comparing the distributed and serial versions, using
the calls

\begin{verbatim}
calm(cls,'ArrDelay ~ DepDelay+Distance,data=yr2008')$tht))  # 45.076
lm(ArrDelay ~ DepDelay+Distance,data=yr2008)))  # 74.472
\end{verbatim}

The run times were 45.076 and 74.472, respectively.  The estimated
regression coefficients were virtually identical:

\begin{verbatim}
-1.061375756  1.019153821 -0.001213223
-1.061369     1.019154    -0.001213
\end{verbatim}

\end{document}


