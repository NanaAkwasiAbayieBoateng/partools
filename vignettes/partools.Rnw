\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.25in}
\setlength{\parindent}{0in}
\setlength{\parskip}{2mm} 

% \usepackage{times}
\usepackage{graphicx}

% library(knitr)
%\VignetteIndexEntry{Partools}

\title{{\bf partools: a Sensible Package for Large Data Sets}}

\author{Norm Matloff, with Contributions from Alex Rumbaugh}

\begin{document}

\maketitle

With the advent of Big Data, the Hadoop framework became ubiquitous.
Yet it was clear that Hadoop had major shortcomings, and recently these
are being much more seriously discussed.\footnote{See for example ``The
Hadoop Honeymoon is Over,'' {\it
https://www.linkedin.com/pulse/hadoop-honeymoon-over-martyn-jones}} This
has resulted in a new platform, Spark, gaining popularity.  As with
Hadoop, there is an R interface available for Spark, named SparkR. 

Spark overcomes one of Hadoop's major problems, which is the lack of
ability to cache data in a multi-pass computation.  However, Spark
unfortunately retains the drawbacks of Hadoop:

\begin{itemize}

\item They are based on a complex, rather opaque infrastructure, and
rely on Java/Scala.  This makes them difficult to install, configure and
use for those who are not computer systems experts. 

\item Although a major plus for Hadoop/Spark is fault tolerance, it is
needed only for users working on extremely large clusters, consisting of
hundreds or thousands of nodes. Disk failure rates are simply too low
for fault tolerance to be an issue for many Hadoop/Snow users, who do
not have such large
systems.\footnote{https://wiki.apache.org/hadoop/PoweredBy} on other
systems.  Worse, the fault tolerance mechanisms, which are especially
extensive in Spark, {\it slow down the computation}.

\end{itemize}

The one firm advantage of Hadoop/Spark is their use of distributed file
systems.  Under the philosophy ``Move the computation to the data,
rather than {\it vice versa},'' network traffic may be greatly reduced,
thus speeding up computation.

Therefore:

\begin{quote} 

It makes sense to develop and use a package that retains the
distributed-file nature of Hadoop/Spark  while staying fully within the
simple, familiar and yet powerful R framework.  

The {\bf partools} package is designed to meet these goals.  It is
intended as {\bf an alternative to Snow/Hadoop}, not necessarily for all
settings, but for many R programmers who are using, or considering using
Hadoop/Snow.

\end{quote}

Since {\bf partools} uses the portion of the R {\bf parallel} package
derived from the package {\bf snow}, and because it is meant as an
alternative to Hadoop, we informally refer to {\bf partools} as
Snowdoop.

Note that if fault tolerance is an issue, one can provide it externally,
say with the XtreemFS system.

\section{Where Is the Magic?}

As you will see later, {\bf partools} can deliver some impressive
speedups.  But there is nothing magical about this.  Instead, the value
of the package consists of just two simple sources:

\begin{itemize}

\item [(a)] The package follows a philosophy of forming distributed
objects {\it and keeping using them in distributed form throughout one's
R session.}

\item [(b)] The package consists of a number of utility functions that
greatly facilitate creating, storing and {\it analyzing} distributed
objects.

\end{itemize}

\section{Overview of the partools Package}

The package is based on the following very simple principles:

\begin{itemize}

\item Files are stored in a distributed manner, in files with a common
basename. For example, the file {\bf x} is stored as separate files {\bf
x.01}, {\bf x.02} etc. 

\item Data frames and matrices are stored in memory at the nodes in a
distributed manner, with a common name.  For example, the data frame
{\bf y} is stored in chunks at the cluster nodes, each chunk known as {\bf
y} at its node.

\end{itemize}

\subsection{Package Structure}

To understand the function structure, and indeed the central philosophy,
keep in mind the following definitions, for a cluster in the R {\bf
parallel} package.

\begin{itemize}

\item A {\it distributed file} {\bf x} consists of a number of separate
physical files, with names {\bf x.01}, {\bf x.02} and so
on.\footnote{The number of leading 0s in the suffix will depend on the
number of cluster nodes.}  The file {\bf x} consists of the {\bf
rbind()} of the rows of those separate files, though {\bf x} may be
virtual.  Cluster node {\bf i} is responsible for {\bf x.i}.

\item A {\it distributed data frame} (or distributed matrix) {\bf d}
consists of a number of separate data frames, one at each cluster node,
all named {\bf d}.  The data frame as a whole consists of the {\bf
rbind()} of the rows of those separate data frames, though it may be
virtual.

\end{itemize}

The fact that all the file chunks or data frame chunks have the same
name (or same prefix) plays a key role in the software.

The package consists of three main groups of functions:

\begin{itemize}

\item {\bf distributed-file functions:}

   \begin{itemize}

   \item {\bf filesplit():}  Create distributed file from monotlithic one.

   \item {\bf filecat():}  Create monotlithic file from distributed one.

   \item {\bf fileread():}  Read distributed file into distributed data
   frame.

   \item {\bf readnscramble():}  Read distributed file into distributed
   data frame, but randomize the record order.

   \item {\bf filesave():}  Write distributed data frame to distributed
   file.

   \item {\bf filechunkname():} For the calling cluster node, returns
   the full name of the file chunk, including suffix, e.g. '01', '02'
   etc.

   \end{itemize}

\item {\bf tabulative functions:}

   \begin{itemize}

   \item {\bf distribsplit():}  Create distributed data frame/matrix
   from monotlithic one.

   \item {\bf distribcat():}  Create monotlithic data frame/matrix from
   distributed one.

   \item {\bf distribagg():}  Distributed form of R's {\bf aggregate()}.

   \item {\bf distribrange():}  Wrapper for {\bf distribagg()} to obtain
   cell counts.

   \item {\bf distribgetrows()}: Applies an R {\bf select()} or similar
   operation to the distributed object, and collects the results into a
   single object at the caller.

   \item {\bf distribrange():}  Distributed form of R's {\bf range()}.

   \end{itemize}

\item {\bf statistical functions:}

These use the Software Alchemy (SA) method ({\it Parallel Computation
for Data Science}, N. Matloff, Chapman and Hall, 2015.) to parallelize
statistical operations.  The idea is simple:  Apply the given estimator
to each chunk in the distributed object, and average over chunks.  It is
proven that the resulting distributed estimator has the same statistical
accuracy as the original serial ones.\footnote{In the world of parallel
computation, the standard word for nonparallel is {\it serial}.}

   \begin{itemize}

   \item {\bf ca():} General SA algorithm.

   \item {\bf cabase():} Core of {\bf ca()}.

   \item {\bf calm():}  Wrapper for SA version of R's {\bf lm()}.

   \item {\bf caglm():}  Wrapper for SA version of R's {\bf glm()}.

   \item {\bf cakm():}  Wrapper for SA version of R's {\bf kmeans()}.

   \item {\bf cakm():}  Wrapper for SA version of R's {\bf aggregate()}.

   \end{itemize}

\end{itemize}

There are also attendant support functions.

\section{Sample Session}

Our data set, from
{\it http://stat-computing.org/dataexpo/2009/the-data.html} consists of
the well-known records of airline flight delay.
For convenience, we'll just use the data for 2008, which consists of
about 7 million records.  This is large enough to illustrate speedup
due to parallelism, but small enough that we won't have to wait really
long amounts of time in our sample session here.  

The session was run on a 16-core machine, with a 16-node {\bf parallel}
cluster.  Note carefully, though, that we should not expect a 16-fold
speedup. In the world of parallel computation, one usually gets of
speedups of considerably less than $n$ for a platform of $n$
computational entities.  Indeed, one is often saddened to find that the
parallel version is actually {\it slower} than the serial one!

The file, {\bf yr2008}, was first split into a distributed file, stored
in {\bf yr2008.01},...,{\bf yr2008.16}, using {\bf filesplitrand()}, and
then read into memory at the 16 cluster nodes using {\bf fileread()}:

\begin{verbatim}
> filesplitrand(cls,'yr2008','yr2008r',2,header=TRUE,sep=",")
> fileread(cls,'yr2008r','yr2008',2,header=TRUE, sep=",")
\end{verbatim}

The call to {\bf filesplitrand()} splits the file as described above;
since these files are permanent, we can skip this step in future R
sessions involving this data (if the file doesn't change).  The function
{\bf filesplitrand()} was used instead of {\bf filesplit()} to construct
the distributed file, in order to randomize the placement of the records
of {\bf yr2008} across cluster nodes.  The advantage of randomization
will be explained shortly.

In order to run timing comparisons, the full file was also read into
memory at the cluster manager:

\begin{verbatim}
> yr2008 <- read.csv("yr2008")
\end{verbatim}

The first operation run involved the package's distributed version of
R's {\bf aggregate()}.  Here we want to tabulate day of week, month and
whether the flight was canceled, broken down into cells according to
flight origin and destination.  We'll find the maximum value in each
cell.

\begin{verbatim}
> system.time(print(distribagg(cls, c("DepDelay","ArrDelay","AirTime"),
   c("Origin","Dest"),"yr2008", FUN="max")))  
...
5193    CDV  YAK      327      325      54
5194    JNU  YAK      317      308      77
5195    SLC  YKM      110      118     115
5196    IPL  YUM      162      163      26
...
   user  system elapsed  
  2.291   0.084  15.952 
\end{verbatim}

\begin{verbatim}
> system.time(print(aggregate(cbind(DepDelay,ArrDelay,AirTime) ~ 
   Origin+Dest,data=yr2008,FUN=max)))  
...
5193    CDV  YAK      327      325      54
5194    JNU  YAK      317      308      77
5195    SLC  YKM      110      118     115
5196    IPL  YUM      162      163      26
...
   user  system elapsed  
249.038   0.444 249.634
\end{verbatim}

So, the results of {\bf distribagg()} did indeed match those of {\bf
aggregate()}, but did so more than 15 times faster!

Remember, the key philosophy of {\bf partools} is to create distributed
objects and then keep using them in distributed form.  However, in some
cases, we may wish to collect a distributed result into a monolithic
object, especially if the result is small.  This is done in the next
example:

Say we wish to do a filter operation, extracting the data on all the
Sunday evening flights, and collect it into one place.   Here is the
direct version:

\begin{verbatim}
> sundayeve <- with(yr2008,yr2008[DayOfWeek==1 & DepTime > 1800,])
\end{verbatim}

This actually is not a time-consuming operation, but again, in typical
{\bf partools} use, we would only have the distributed version of {\bf
yr2008}.  Here is how we would achieve the same effect from the
distributed object:

\begin{verbatim}
> sundayeved <- distribgetrows(cls,'with(yr2008,yr2008[DayOfWeek==1 & DepTime > 1800,])')
\end{verbatim}

In, say, investigating data accuracy, we may wish to flag all records
having an inordinate number of NA values.  As I first step, we may wish
to add a column to our data frame, indicating how many NA values are in
each row.  If we did not have the advantage of distributed computation,
here is how long it would take for our flight delay data:

\begin{verbatim}
> sumna            
function(x) sum(is.na(x))
> system.time(yr2008$n1 <- apply(yr2008[,c(5,7,8,11:16,19:21)],1,sumna))
   user  system elapsed
268.463   0.773 269.542
\end{verbatim}

But it is of course much faster on a distributed basis, using the {\bf
parallel} package function {\bf clusterEvalQ()}:

\begin{verbatim}
> clusterExport(cls,"sumna",envir=environment())
> system.time(clusterEvalQ(cls,yr2008$n1 <- apply(yr2008[,c(5,7,8,11:16,19:21)],1,sumna)))
   user  system elapsed 
  0.094   0.012  16.758 
\end{verbatim}

The speedup here was about 16, fully utilizing all 16 cores.

Ordinarily, we would continue that NA analysis on a distributed basis,
in keeping with the {\bf partools} philosophy of setting up distributed
objects and then repeatedly dealing with them on a distributed basis.
If our subsequent operations continue to have time complexity linear in
the number of records processes, we should continue to get speedups of
about 16.

On the other hand, we may wish to gather together all the records have 8
or more NA values.  In the nonparallel context, it would take some time:

\begin{verbatim}
> system.time(na8 <- yr2008[yr2008$n1 > 7,])
   user  system elapsed 
  9.292   0.028   9.327 
\end{verbatim}

In the distributed manner, it is slightly faster:

\begin{verbatim}
> system.time(na8d <- distribgetrows(cls,'yr2008[yr2008$n1 > 7,]'))
   user  system elapsed 
  5.524   0.160   6.584 
\end{verbatim}

The speedup is less here, as the resulting data must travel from the
cluster nodes to the cluster manager. In this case, this is just a
memory-to-memory transfer rather than across a netwrok, as we are on a
multicore machine, but it still takes time.  If the number of records
satisfying the filtering condition had been smaller than the 136246 we
had here, the speedup factor would have been greater.

Now let's turn to statistical operations, starting of course with linear
regression.  As noted, these {\bf partools} functions make use of
Software Alchemy, which replaces the given operation by a {\it
distributed, statistically equivalent} operation.  This will often
produce a significant speedup.  Note again that though the result may
different from the non-distributed version, say in the third significant
digit, it is just as accurate statistically.

In the flight data, we predicted the arrival delay from the departure
delay and distance, comparing the distributed and serial versions, 

\begin{verbatim}
> system.time(print(lm(ArrDelay ~ DepDelay+Distance,data=yr2008)))  
...     
Coefficients:
(Intercept)     DepDelay     Distance  
  -1.061369     1.019154    -0.001213  

   user  system elapsed 
 77.107  12.463  76.225 
> system.time(print(calm(cls,'ArrDelay ~ DepDelay+Distance,data=yr2008')$tht))  
 (Intercept)     DepDelay     Distance 
-1.061262941  1.019150592 -0.001213252 
   user  system elapsed 
 13.414   0.691  18.396 
\end{verbatim}

Linear regression is very hard to parallelize, so the speedup of more than
4 here is nice.  Coefficient estimates were virtually identical.

Next, principal components.  Since R's {\bf prcomp()} does not handle NA
values for nonformula specifications, let's do that separately first:

\begin{verbatim}
> system.time(cc <- na.omit(yr2008[,c(12:16,19:21)]))
   user  system elapsed 
  9.540   0.351   9.907 
> system.time(clusterEvalQ(cls,cc <- na.omit(yr2008[,c(12:16,19:21)])))
   user  system elapsed 
  0.885   0.232   2.352 
\end{verbatim}

Note that this too was faster in the distributed approach, though both
times were small.  And now the PCA runs:

\begin{verbatim}
> system.time(ccout <- prcomp(cc))
   user  system elapsed 
 61.905  49.605  58.444 
> ccout$sdev
[1] 5.752546e+02 5.155227e+01 2.383117e+01 1.279210e+01 9.492825e+00
[6] 5.530152e+00 1.133015e-03 6.626621e-12
> system.time(ccoutdistr <- caprcomp(cls,'cc',8))
   user  system elapsed 
  5.023   0.604   8.949 
> ccoutdistr$sdev 
[1] 5.752554e+02 5.155127e+01 2.383122e+01 1.279184e+01 9.492570e+00
[6] 5.529869e+00 9.933142e-04 8.679427e-13
\end{verbatim}

Thus, more than a 6-fold speedup here.  Agreement of the component
standard deviations is good. 

Last, let's find the {\it interquartile range} for several columns.
This is a robust measure of dispersion, defined at the difference
between the 75$^{th}$ and 25$^{th}$ percentiles. 

Here is serial code to find this:

\begin{verbatim}
# find the interquartile range for a vector x
iqr <- function(x) {tmp <- quantile(x,na.rm=T); tmp[4] - tmp[2]}
# find the interquartile range for each column of a data frame dfr
iqrm <- function(dfr) apply(dfr,2,iqr)
\end{verbatim}

So, let's compare times.  First, the serial version:

\begin{verbatim}
> system.time(print(iqrm(yr2008[,c(5:8,12:16,19:21)])))
          DepTime        CRSDepTime           ArrTime        CRSArrTime 
              800               790               802               792 
ActualElapsedTime    CRSElapsedTime           AirTime          ArrDelay 
               80                79                77                22 
         DepDelay          Distance            TaxiIn           TaxiOut 
               12               629                 4                 9 
   user  system elapsed 
 29.280   0.243  29.554 
\end{verbatim}

For the distributed version,

\begin{verbatim}
> system.time(print(colMeans(distribgetrows(cls,'iqrm(yr2008[,c(5:8,12:16,19:21)])'))))
          DepTime        CRSDepTime           ArrTime        CRSArrTime 
         800.1250          790.0625          801.8125          791.8750 
ActualElapsedTime    CRSElapsedTime           AirTime          ArrDelay 
          80.0000           78.9375           76.5625           22.0000 
         DepDelay          Distance            TaxiIn           TaxiOut 
          12.0000          627.6875            4.0000            9.0000 
   user  system elapsed 
  0.009   0.002   2.587 
\end{verbatim}

Here the speedup was more than 11-fold, with agreement generally to
three significant digits.  Once again, note that statistically speaking,
both estimators have the same accuracy.

\end{document}


