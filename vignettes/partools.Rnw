\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{0.0in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.25in}
\setlength{\parindent}{0in}
\setlength{\parskip}{2mm} 

% \usepackage{times}
\usepackage{graphicx}

% library(knitr)
%\VignetteIndexEntry{Partools}

\title{partools: a Sensiible Package for Large Data Sets}

\author{Norm Matloff, with Contributions from Alex Rumbaugh}

\begin{document}

\maketitle

With the advent of Big Data, the Hadoop framework became ubiquitous.
Yet it was clear that Hadoop had major shortcomings, and recently these
are being much more seriously discussed.\footnote{See for example ``The
Hadoop Honeymoon is Over,'' {\it
https://www.linkedin.com/pulse/hadoop-honeymoon-over-martyn-jones}} This
has resulted in a new platform, Spark, gaining popularity.  As with
Hadoop, there is an R interfaces available for Spark, named SparkR. 

Spark overcomes one of Hadoop's major problems, which is the lack of
ability to cache data in a multi-pass computation.  However, Spark
unfortunately retains the drawbacks of Hadoop:

\begin{itemize}

\item Due to reliance on sophisticated infrastructure involving Java and
Scalar, they are difficult to install and properly configure for those
who are not computer systems experts. 

\item A major plus for Hadoop/Spark, fault tolerance, is needed only for
users working on extremely large clusters, consisting of hundreds or
thousands of nodes. Disk failure rates are simply too low for fault
tolerance to be an issue on other systems.\footnote{In addition, one can
provide fault tolerance externally, say with the XtreemFS system.}
Worse, the fault tolerance mechanisms, which are especially extensive in
Spark, {\it slow down the computation}.

\end{itemize}

The one firm advantage of Hadoop/Spark is their use of distributed file
systems.  Under the philosophy ``Move the computation to the data,
rather than {\it vice versa},'' network traffic may be greatly reduced,
thus speeding up computation.

Therefore:

\begin{quote}
It makes sense to develop and use a package that retains the
distributed-file nature of Hadoop/Spark  while staying fully within 
the simple-yet-more-powerful R framework.  The {\bf partools} package
is designed to meet these goals.
\end{quote}

Since {\bf partools} uses the portion of the R {\bf parallel} package
derived from the package {\bf snow}, and because it is meant as an
alternative to Hadoop, we informally refer to {\bf partools} as
Snowdoop.

\section{Overview of the partools Package}

The package is based on the following very simple principles:

\begin{itemize}

\item Files are stored in a distributed manner, in files with a common
basename. For example, the file {\bf x} is stored as separate files {\bf
x.01}, {\bf x.02} etc. 

\item Data frames and matrices are stored in memory at the nodes in a
distributed manner, with a common name.  For example, the data frame
{\bf y} is stored in chunks at the cluster nodes, each chunk known as {\bf
y} at its node.

\end{itemize}

\subsection{Package Structure}

To understand the function structure, and indeed the central philosophy,
keep in mind the following definitions, for a cluster in the R {\bf
parallel} package.

\begin{itemize}

\item A {\it distributed file} {\bf x} consists of a number of separate
physical files, with names {\bf x.01}, {\bf x.02} and so
on.\footnote{The number of leading 0s in the suffix will depend on the
number of cluster nodes.}  The file {\bf x} consists of the {\bf
rbind()} of the rows of those separate files, though {\bf x} may be
virtual.  Cluster node {\bf i} is responsible for {\bf x.i}.

\item A {\it distributed data frame} (or distributed matrix) {\bf d}
consists of a number of separate data frames, one at each cluster node,
all named {\bf d}.  The data frame as a whole consists of the {\bf
rbind()} of the rows of those separate data frames, though it may be
virtual.

\end{itemize}

The fact that all the file chunks or data frame chunks have the same
name (or same prefix) plays a key role in the software.

The package consists of three main groups of functions:

\begin{itemize}

\item {\bf distributed-file functions:}

   \begin{itemize}

   \item {\bf filesplit():}  Create distributed file from monotlithic one.

   \item {\bf filecat():}  Create monotlithic file from distributed one.

   \item {\bf fileread():}  Read distributed file into distributed data
   frame.

   \item {\bf readnscramble():}  Read distributed file into distributed
   data frame, but randomize the record order.

   \item {\bf filesave():}  Write distributed data frame to distributed
   file.

   \end{itemize}

\item {\bf tabulative functions:}

   \begin{itemize}

   \item {\bf distribsplit():}  Create distributed data frame/matrix
   from monotlithic one.

   \item {\bf distribcat():}  Create monotlithic data frame/matrix from
   distributed one.

   \item {\bf distribagg():}  Distributed form of R's {\bf aggregate()}.

   \item {\bf distribrange():}  Wrapper for {\bf distribagg()} to obtain
   cell counts.

   \item {\bf distribgetrows()}: Applies an R {\bf select()} or similar
   operation to the distributed object, and collects the results into a
   single object at the caller.

   \item {\bf distribrange():}  Distributed form of R's {\bf range()}.

   \end{itemize}

\item {\bf statistical functions:}

These use the Software Alchemy (SA) method ({\it Parallel Computation for
Data Science}, N. Matloff, Chapman and Hall, 2015.) to parallelize
statistical operations.  The resulting estimators have the same
statistical accuracy as the original serial ones.

   \begin{itemize}

   \item {\bf ca():} General SA algorithm.

   \item {\bf cabase():} Core of {\bf ca()}.

   \item {\bf calm():}  Wrapper for SA version of R's {\bf lm()}.

   \item {\bf caglm():}  Wrapper for SA version of R's {\bf glm()}.

   \item {\bf cakm():}  Wrapper for SA version of R's {\bf kmeans()}.

   \item {\bf cakm():}  Wrapper for SA version of R's {\bf aggregate()}.

   \end{itemize}

\end{itemize}

There are also attendant support functions.

\section{Sample Session}

The example here involves the well-known airline flight delay data,
available at {\it http://stat-computing.org/dataexpo/2009/the-data.html.
For convenience, we'll just use the data for 2008, which consists of
about 7 million records.}  This is large enough to illustrate speedup due
to parallelism, but small enough that we won't have to wait really long
amounts of time in our sample session here.


\end{document}

